"""
BAXperience Clustering Processor
===============================

Implementa algoritmos de clustering para POIs y an√°lisis de datos geogr√°ficos.

- Clustering geogr√°fico (K-means) para POIs
- Clustering por categor√≠as y caracter√≠sticas  
- An√°lisis de densidad por barrios
- Detecci√≥n de zonas tur√≠sticas
- M√©tricas de calidad de clusters

Autor: BAXperience Team
"""

import logging
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, DBSCAN
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import psycopg2
from psycopg2.extras import RealDictCursor
from typing import Dict, List, Tuple, Optional
import json
from datetime import datetime

logger = logging.getLogger(__name__)

class ClusteringProcessor:
    """Procesador de clustering para POIs y an√°lisis geogr√°fico"""
    
    def __init__(self, proc_db_config: Dict):
        """
        Inicializar procesador de clustering
        
        Args:
            proc_db_config: Configuraci√≥n de BD Data Processor
        """
        self.proc_db_config = proc_db_config
        self.proc_conn = None
        self.results = {}
        
    def connect_processor_db(self):
        """Conectar a BD Data Processor"""
        try:
            self.proc_conn = psycopg2.connect(**self.proc_db_config)
            logger.info("Conectado a BD Data Processor para clustering")
        except Exception as e:
            logger.error(f"Error conectando a BD Data Processor: {e}")
            raise
    
    def disconnect_processor_db(self):
        """Desconectar de BD Data Processor"""
        if self.proc_conn:
            self.proc_conn.close()
            logger.info("Desconectado de BD Data Processor")
    
    def load_pois_data(self) -> pd.DataFrame:
        """Cargar datos de POIs para clustering"""
        logger.info("Cargando datos de POIs para clustering...")
        
        query = """
        SELECT 
            id, nombre, categoria, subcategoria,
            latitud, longitud, barrio, comuna,
            valoracion_promedio, numero_valoraciones,
            tipo_cocina, tipo_ambiente
        FROM lugares_clustering
        WHERE latitud IS NOT NULL AND longitud IS NOT NULL
        ORDER BY id
        """
        
        cursor = self.proc_conn.cursor(cursor_factory=RealDictCursor)
        cursor.execute(query)
        data = cursor.fetchall()
        cursor.close()
        
        if not data:
            logger.warning("No se encontraron POIs para clustering")
            return pd.DataFrame()
        
        df = pd.DataFrame(data)
        logger.info(f"Cargados {len(df)} POIs para clustering")
        return df
    
    def find_optimal_clusters(self, coords_scaled: np.ndarray, max_k: int = 15) -> int:
        """
        Encuentra el n√∫mero √≥ptimo de clusters usando Silhouette score
        
        Args:
            coords_scaled: Coordenadas normalizadas
            max_k: N√∫mero m√°ximo de clusters a probar
            
        Returns:
            N√∫mero √≥ptimo de clusters
        """
        logger.info("Determinando n√∫mero √≥ptimo de clusters...")
        
        if len(coords_scaled) < 4:
            return min(2, len(coords_scaled))
        
        # Limitar max_k al n√∫mero de puntos disponibles
        max_k = min(max_k, len(coords_scaled) - 1)
        
        silhouette_scores = []
        k_range = range(2, min(max_k + 1, 20))  # Limitar a 20 para eficiencia
        
        for k in k_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=20, max_iter=500)
            labels = kmeans.fit_predict(coords_scaled)
            score = silhouette_score(coords_scaled, labels)
            silhouette_scores.append(score)
            logger.debug(f"K={k}, Silhouette={score:.3f}")
        
        # Seleccionar k con mejor silhouette score
        if not silhouette_scores:
            return 2
        
        optimal_idx = np.argmax(silhouette_scores)
        optimal_k = list(k_range)[optimal_idx]
        best_score = silhouette_scores[optimal_idx]
        
        logger.info(f"N√∫mero √≥ptimo de clusters determinado: {optimal_k} (Silhouette: {best_score:.3f})")
        return optimal_k
    
    def _find_optimal_hierarchical_clusters(self, coords_scaled: np.ndarray, max_k: int = 12) -> int:
        """
        Encuentra el n√∫mero √≥ptimo de clusters para hierarchical usando Silhouette score
        
        Args:
            coords_scaled: Coordenadas normalizadas
            max_k: N√∫mero m√°ximo de clusters a probar
            
        Returns:
            N√∫mero √≥ptimo de clusters
        """
        logger.info("Determinando n√∫mero √≥ptimo de clusters jer√°rquicos...")
        
        if len(coords_scaled) < 4:
            return min(2, len(coords_scaled))
        
        # Limitar max_k al n√∫mero de puntos disponibles y a un m√°ximo razonable
        max_k = min(max_k, len(coords_scaled) - 1, 12)
        
        # Probar diferentes linkages y quedarse con el mejor
        best_linkage = 'ward'
        best_score_overall = -1
        best_k_overall = 2
        
        for linkage_method in ['ward', 'complete', 'average']:
            silhouette_scores = []
            k_range = range(2, max_k + 1)
            
            for k in k_range:
                hierarchical = AgglomerativeClustering(n_clusters=k, linkage=linkage_method)
                labels = hierarchical.fit_predict(coords_scaled)
                score = silhouette_score(coords_scaled, labels)
                silhouette_scores.append(score)
            
            # Encontrar mejor k para este linkage
            if silhouette_scores:
                max_score = max(silhouette_scores)
                if max_score > best_score_overall:
                    best_score_overall = max_score
                    best_k_overall = list(k_range)[silhouette_scores.index(max_score)]
                    best_linkage = linkage_method
        
        logger.info(f"Mejor linkage: {best_linkage}")
        
        logger.info(f"N√∫mero √≥ptimo de clusters jer√°rquicos: {best_k_overall} (Silhouette: {best_score_overall:.3f})")
        
        # Guardar el mejor linkage para usar en la ejecuci√≥n final
        self.best_hierarchical_linkage = best_linkage
        return best_k_overall

    def geographic_clustering(self, df: pd.DataFrame, n_clusters: Optional[int] = None) -> Dict:
        """
        Clustering geogr√°fico usando K-means con detecci√≥n autom√°tica del n√∫mero √≥ptimo
        
        Args:
            df: DataFrame con POIs
            n_clusters: N√∫mero espec√≠fico de clusters (opcional, se auto-detecta si es None)
            
        Returns:
            Dict con resultados del clustering
        """
        if df.empty:
            return {'status': 'no_data', 'clusters': 0}
        
        # Preparar datos geogr√°ficos
        coords = df[['latitud', 'longitud']].astype(float).values
        
        # Normalizar coordenadas usando MinMaxScaler (mejor para datos geogr√°ficos)
        scaler = MinMaxScaler()
        coords_scaled = scaler.fit_transform(coords)
        
        # Determinar n√∫mero √≥ptimo de clusters si no se especifica
        if n_clusters is None:
            n_clusters = self.find_optimal_clusters(coords_scaled)
        
        logger.info(f"Ejecutando K-Means geogr√°fico con {n_clusters} clusters...")
        
        # K-means clustering con m√°s iteraciones para mejor convergencia
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20, max_iter=500)
        cluster_labels = kmeans.fit_predict(coords_scaled)
        
        # Calcular m√©tricas de calidad
        silhouette = silhouette_score(coords_scaled, cluster_labels)
        
        # Agregar clusters al DataFrame
        df_clustered = df.copy()
        df_clustered['cluster_geografico'] = cluster_labels
        
        # Calcular estad√≠sticas por cluster
        cluster_stats = []
        for i in range(n_clusters):
            cluster_data = df_clustered[df_clustered['cluster_geografico'] == i]
            
            if len(cluster_data) > 0:
                # Centroide real (promedio de coordenadas)
                centroide_lat = cluster_data['latitud'].mean()
                centroide_lng = cluster_data['longitud'].mean()
                
                # Estad√≠sticas del cluster
                stats = {
                    'cluster_id': int(i),
                    'num_pois': len(cluster_data),
                    'centroide_lat': float(centroide_lat),
                    'centroide_lng': float(centroide_lng),
                    'categorias': cluster_data['categoria'].value_counts().to_dict(),
                    'barrios': cluster_data['barrio'].value_counts().to_dict(),
                    'valoracion_promedio': float(cluster_data['valoracion_promedio'].mean()) if cluster_data['valoracion_promedio'].notna().any() else 0.0,
                    'radius_km': self._calculate_cluster_radius(cluster_data)
                }
                cluster_stats.append(stats)
        
        # Crear mapeo poi_id -> cluster_id para usar en recomendaciones
        poi_clusters = {}
        for idx, row in df_clustered.iterrows():
            poi_id = int(row['id'])  # id de lugares_clustering
            cluster_id = int(row['cluster_geografico'])
            poi_clusters[poi_id] = cluster_id
        
        # Crear diccionario de centroides para c√°lculo de cluster de origen
        cluster_centers = {}
        for stats in cluster_stats:
            cluster_id = stats['cluster_id']
            cluster_centers[cluster_id] = [stats['centroide_lat'], stats['centroide_lng']]
        
        results = {
            'status': 'success',
            'algorithm': 'kmeans',
            'n_clusters': n_clusters,
            'silhouette_score': float(silhouette),
            'total_pois': len(df),
            'cluster_stats': cluster_stats,
            'cluster_centers': cluster_centers,  # ‚Üê NUEVO: centroides para diversidad geogr√°fica
            'poi_clusters': poi_clusters,  # ‚Üê AGREGADO: mapeo para recomendaciones
            'dataframe': df_clustered
        }
        
        logger.info(f"Clustering geogr√°fico completado. Silhouette score: {silhouette:.3f}")
        return results
    
    def dbscan_clustering(self, df: pd.DataFrame, eps: float = 0.01, min_samples: int = 3) -> Dict:
        """
        Clustering usando DBSCAN para detectar clusters de densidad variable
        
        Args:
            df: DataFrame con POIs
            eps: Radio m√°ximo entre puntos en un cluster
            min_samples: M√≠nimo n√∫mero de puntos para formar un cluster
            
        Returns:
            Dict con resultados del clustering DBSCAN
        """
        logger.info(f"Ejecutando clustering DBSCAN (eps={eps}, min_samples={min_samples})...")
        
        if df.empty:
            return {'status': 'no_data', 'clusters': 0}
        
        # Preparar datos geogr√°ficos
        coords = df[['latitud', 'longitud']].astype(float).values
        
        # Normalizar coordenadas
        scaler = StandardScaler()
        coords_scaled = scaler.fit_transform(coords)
        
        # DBSCAN clustering
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        cluster_labels = dbscan.fit_predict(coords_scaled)
        
        # Agregar clusters al DataFrame
        df_clustered = df.copy()
        df_clustered['cluster_dbscan'] = cluster_labels
        
        # An√°lisis de resultados
        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        n_noise = list(cluster_labels).count(-1)
        
        # Calcular silhouette solo si hay m√°s de 1 cluster
        silhouette = 0.0
        if n_clusters > 1:
            # Filtrar ruido para silhouette
            mask = cluster_labels != -1
            if np.sum(mask) > 1:
                silhouette = silhouette_score(coords_scaled[mask], cluster_labels[mask])
        
        # Estad√≠sticas por cluster
        cluster_stats = []
        for i in set(cluster_labels):
            if i == -1:  # Ruido
                continue
                
            cluster_data = df_clustered[df_clustered['cluster_dbscan'] == i]
            if len(cluster_data) > 0:
                stats = {
                    'cluster_id': int(i),
                    'num_pois': len(cluster_data),
                    'centroide_lat': float(cluster_data['latitud'].mean()),
                    'centroide_lng': float(cluster_data['longitud'].mean()),
                    'categorias': cluster_data['categoria'].value_counts().to_dict(),
                    'density_score': len(cluster_data) / max(1, len(df))  # Densidad relativa
                }
                cluster_stats.append(stats)
        
        # Crear mapeo poi_id -> cluster_id para usar en recomendaciones
        poi_clusters = {}
        for idx, row in df_clustered.iterrows():
            poi_id = int(row['id'])  # id de lugares_clustering
            cluster_id = int(row['cluster_dbscan'])
            poi_clusters[poi_id] = cluster_id
        
        results = {
            'status': 'success',
            'algorithm': 'dbscan',
            'n_clusters': n_clusters,
            'n_noise': n_noise,
            'noise_ratio': n_noise / len(df) if len(df) > 0 else 0,
            'silhouette_score': float(silhouette),
            'cluster_stats': cluster_stats,
            'poi_clusters': poi_clusters,  # ‚Üê AGREGADO: mapeo para recomendaciones
            'dataframe': df_clustered
        }
        
        logger.info(f"DBSCAN completado. Clusters: {n_clusters}, Ruido: {n_noise}")
        return results
    
    def hierarchical_clustering(self, df: pd.DataFrame, n_clusters: Optional[int] = None) -> Dict:
        """
        Clustering jer√°rquico aglomerativo con detecci√≥n autom√°tica del n√∫mero √≥ptimo
        
        Args:
            df: DataFrame con POIs
            n_clusters: N√∫mero espec√≠fico de clusters (opcional, se auto-detecta si es None)
            
        Returns:
            Dict con resultados del clustering jer√°rquico
        """
        if df.empty:
            return {'status': 'no_data', 'clusters': 0}
        
        # Preparar datos geogr√°ficos
        coords = df[['latitud', 'longitud']].astype(float).values
        
        # Normalizar coordenadas usando MinMaxScaler (mejor para datos geogr√°ficos)
        scaler = MinMaxScaler()
        coords_scaled = scaler.fit_transform(coords)
        
        # Ponderar latitud para hierarchical (Buenos Aires es m√°s vertical)
        # Esto mejora la separaci√≥n norte-sur en clustering jer√°rquico
        coords_scaled[:, 0] *= 1.5  # Latitud (eje vertical) x1.5
        
        # Determinar n√∫mero √≥ptimo de clusters si no se especifica
        if n_clusters is None:
            n_clusters = self._find_optimal_hierarchical_clusters(coords_scaled)
        
        # Usar el mejor linkage encontrado, o 'ward' por defecto
        best_linkage = getattr(self, 'best_hierarchical_linkage', 'ward')
        logger.info(f"Ejecutando clustering jer√°rquico con {n_clusters} clusters (linkage={best_linkage})...")
        
        # Clustering jer√°rquico con mejor linkage
        hierarchical = AgglomerativeClustering(n_clusters=n_clusters, linkage=best_linkage)
        cluster_labels = hierarchical.fit_predict(coords_scaled)
        
        # Calcular m√©tricas
        silhouette = silhouette_score(coords_scaled, cluster_labels)
        
        # Agregar clusters al DataFrame
        df_clustered = df.copy()
        df_clustered['cluster_hierarchical'] = cluster_labels
        
        # Estad√≠sticas por cluster
        cluster_stats = []
        for i in range(n_clusters):
            cluster_data = df_clustered[df_clustered['cluster_hierarchical'] == i]
            
            if len(cluster_data) > 0:
                stats = {
                    'cluster_id': int(i),
                    'num_pois': len(cluster_data),
                    'centroide_lat': float(cluster_data['latitud'].mean()),
                    'centroide_lng': float(cluster_data['longitud'].mean()),
                    'categorias': cluster_data['categoria'].value_counts().to_dict(),
                    'compactness': self._calculate_cluster_compactness(cluster_data)
                }
                cluster_stats.append(stats)
        
        # Crear mapeo poi_id -> cluster_id para usar en recomendaciones
        poi_clusters = {}
        for idx, row in df_clustered.iterrows():
            poi_id = int(row['id'])  # id de lugares_clustering
            cluster_id = int(row['cluster_hierarchical'])
            poi_clusters[poi_id] = cluster_id
        
        # Crear relaciones entre categor√≠as basadas en clusters jer√°rquicos
        category_relationships = self._calculate_category_relationships(df_clustered)
        
        results = {
            'status': 'success',
            'algorithm': 'hierarchical',
            'n_clusters': n_clusters,
            'silhouette_score': float(silhouette),
            'cluster_stats': cluster_stats,
            'poi_clusters': poi_clusters,  # ‚Üê AGREGADO: mapeo para recomendaciones
            'category_relationships': category_relationships,  # ‚Üê AGREGADO: relaciones entre categor√≠as
            'dataframe': df_clustered
        }
        
        logger.info(f"Clustering jer√°rquico completado. Silhouette score: {silhouette:.3f}")
        return results
    
    def _calculate_cluster_compactness(self, cluster_data: pd.DataFrame) -> float:
        """
        Calcula la compacidad de un cluster (qu√© tan agrupados est√°n los puntos)
        """
        if len(cluster_data) < 2:
            return 1.0
        
        coords = cluster_data[['latitud', 'longitud']].astype(float).values
        center = coords.mean(axis=0)
        
        # Distancia promedio al centro
        distances = np.sqrt(((coords - center) ** 2).sum(axis=1))
        return 1.0 / (1.0 + distances.mean())  # Invertir para que mayor compacidad = mayor valor

    def category_clustering(self, df: pd.DataFrame) -> Dict:
        """
        Clustering por categor√≠as y caracter√≠sticas
        
        Args:
            df: DataFrame con POIs
            
        Returns:
            Dict con an√°lisis por categor√≠as
        """
        logger.info("Ejecutando an√°lisis de clustering por categor√≠as...")
        
        if df.empty:
            return {'status': 'no_data'}
        
        # An√°lisis por categor√≠a
        category_analysis = {}
        
        for categoria in df['categoria'].unique():
            if pd.isna(categoria):
                continue
                
            cat_data = df[df['categoria'] == categoria]
            
            analysis = {
                'total_pois': len(cat_data),
                'barrios_distribution': cat_data['barrio'].value_counts().head(10).to_dict(),
                'subcategorias': cat_data['subcategoria'].value_counts().to_dict(),
                'valoracion_promedio': float(cat_data['valoracion_promedio'].mean()) if cat_data['valoracion_promedio'].notna().any() else 0.0,
                'densidade_geografica': self._calculate_geographic_density(cat_data)
            }
            
            # An√°lisis espec√≠fico para gastronom√≠a
            if categoria.lower() == 'gastronom√≠a':
                analysis['tipos_cocina'] = cat_data['tipo_cocina'].value_counts().head(10).to_dict()
                analysis['tipos_ambiente'] = cat_data['tipo_ambiente'].value_counts().head(10).to_dict()
            
            # An√°lisis espec√≠fico para monumentos (campos disponibles limitados)
            elif categoria.lower() == 'monumentos':
                analysis['subcategorias_detalle'] = cat_data['subcategoria'].value_counts().head(10).to_dict()
            
            category_analysis[categoria] = analysis
        
        results = {
            'status': 'success',
            'total_categories': len(category_analysis),
            'category_analysis': category_analysis
        }
        
        logger.info(f"An√°lisis por categor√≠as completado: {len(category_analysis)} categor√≠as")
        return results
    
    def neighborhood_clustering(self, df: pd.DataFrame) -> Dict:
        """
        An√°lisis de clusters por barrios
        
        Args:
            df: DataFrame con POIs
            
        Returns:
            Dict con an√°lisis por barrios
        """
        logger.info("Ejecutando an√°lisis de clustering por barrios...")
        
        if df.empty:
            return {'status': 'no_data'}
        
        neighborhood_analysis = {}
        
        for barrio in df['barrio'].unique():
            if pd.isna(barrio):
                continue
            
            barrio_data = df[df['barrio'] == barrio]
            
            if len(barrio_data) < 2:  # Muy pocos POIs para an√°lisis
                continue
            
            analysis = {
                'total_pois': len(barrio_data),
                'densidad_poi_km2': self._estimate_poi_density(barrio_data),
                'categorias_distribution': barrio_data['categoria'].value_counts().to_dict(),
                'valoracion_promedio': float(barrio_data['valoracion_promedio'].mean()) if barrio_data['valoracion_promedio'].notna().any() else 0.0,
                'centroide_lat': float(barrio_data['latitud'].astype(float).mean()),
                'centroide_lng': float(barrio_data['longitud'].astype(float).mean()),
                'diversidad_categoria': len(barrio_data['categoria'].unique()),
                'poi_mejor_valorado': self._get_best_poi(barrio_data)
            }
            
            neighborhood_analysis[barrio] = analysis
        
        # Ranking de barrios por diferentes m√©tricas
        rankings = self._calculate_neighborhood_rankings(neighborhood_analysis)
        
        results = {
            'status': 'success',
            'total_neighborhoods': len(neighborhood_analysis),
            'neighborhood_analysis': neighborhood_analysis,
            'rankings': rankings
        }
        
        logger.info(f"An√°lisis por barrios completado: {len(neighborhood_analysis)} barrios")
        return results
    
    def detect_tourist_zones(self, geographic_results: Dict, category_results: Dict) -> Dict:
        """
        Detectar zonas tur√≠sticas basado en clustering
        
        Args:
            geographic_results: Resultados de clustering geogr√°fico
            category_results: Resultados de clustering por categor√≠as
            
        Returns:
            Dict con zonas tur√≠sticas detectadas
        """
        logger.info("Detectando zonas tur√≠sticas...")
        
        if geographic_results.get('status') != 'success':
            return {'status': 'no_data'}
        
        tourist_zones = []
        
        for cluster in geographic_results['cluster_stats']:
            cluster_id = cluster['cluster_id']
            
            # Criterios para zona tur√≠stica:
            # 1. Diversidad de categor√≠as
            # 2. Alta densidad de POIs
            # 3. Buena valoraci√≥n promedio
            
            num_categories = len(cluster['categorias'])
            poi_density = cluster['num_pois']
            avg_rating = cluster['valoracion_promedio']
            
            # Puntaje tur√≠stico (0-100)
            tourist_score = 0
            
            # Diversidad de categor√≠as (0-30 puntos)
            tourist_score += min(num_categories * 5, 30)
            
            # Densidad de POIs (0-30 puntos)  
            tourist_score += min(poi_density * 2, 30)
            
            # Valoraci√≥n promedio (0-40 puntos)
            tourist_score += avg_rating * 8
            
            if tourist_score >= 50:  # Umbral para zona tur√≠stica
                zone = {
                    'cluster_id': cluster_id,
                    'tourist_score': round(tourist_score, 1),
                    'centroide_lat': cluster['centroide_lat'],
                    'centroide_lng': cluster['centroide_lng'],
                    'num_pois': cluster['num_pois'],
                    'diversidad_categorias': num_categories,
                    'categorias_principales': list(cluster['categorias'].keys())[:3],
                    'barrios_incluidos': list(cluster['barrios'].keys()),
                    'radius_km': cluster['radius_km'],
                    'descripcion': self._generate_zone_description(cluster)
                }
                tourist_zones.append(zone)
        
        # Ordenar por puntaje tur√≠stico
        tourist_zones.sort(key=lambda x: x['tourist_score'], reverse=True)
        
        results = {
            'status': 'success',
            'total_zones': len(tourist_zones),
            'tourist_zones': tourist_zones,
            'algorithm_params': {
                'min_tourist_score': 50,
                'diversity_weight': 0.3,
                'density_weight': 0.3,
                'rating_weight': 0.4
            }
        }
        
        logger.info(f"Detectadas {len(tourist_zones)} zonas tur√≠sticas")
        return results
    
    def save_clustering_results(self, results: Dict):
        """
        Guardar resultados de clustering en BD
        
        Args:
            results: Diccionario con todos los resultados
        """
        logger.info("Guardando resultados de clustering en BD...")
        
        cursor = self.proc_conn.cursor()
        
        try:
            # Crear tabla de resultados de clustering si no existe
            create_table_sql = """
            CREATE TABLE IF NOT EXISTS clustering_results (
                id SERIAL PRIMARY KEY,
                algorithm_type VARCHAR(50) NOT NULL,
                results_json JSONB NOT NULL,
                silhouette_score DECIMAL(5,3),
                n_clusters INTEGER,
                total_pois INTEGER,
                fecha_calculo TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
            
            CREATE INDEX IF NOT EXISTS idx_clustering_type ON clustering_results(algorithm_type);
            CREATE INDEX IF NOT EXISTS idx_clustering_fecha ON clustering_results(fecha_calculo);
            """
            cursor.execute(create_table_sql)
            
            # Limpiar resultados anteriores del mismo d√≠a
            cursor.execute("""
                DELETE FROM clustering_results 
                WHERE algorithm_type IN ('geographic', 'category', 'neighborhood', 'tourist_zones')
                AND DATE(fecha_calculo) = CURRENT_DATE
            """)
            
            # Insertar resultados
            for algorithm_type, result_data in results.items():
                if result_data.get('status') == 'success':
                    # Limpiar DataFrame antes de guardar
                    clean_results = result_data.copy()
                    if 'dataframe' in clean_results:
                        del clean_results['dataframe']
                    
                    # Convertir tipos numpy a tipos nativos de Python
                    clean_results = self._convert_numpy_types(clean_results)
                    
                    insert_sql = """
                    INSERT INTO clustering_results 
                    (algorithm_type, results_json, silhouette_score, n_clusters, total_pois)
                    VALUES (%s, %s, %s, %s, %s)
                    """
                    
                    params = (
                        algorithm_type,
                        json.dumps(clean_results, default=str, ensure_ascii=False),
                        float(result_data.get('silhouette_score', 0)),
                        int(result_data.get('n_clusters', 0)),
                        int(result_data.get('total_pois', 0))
                    )
                    
                    cursor.execute(insert_sql, params)
            
            self.proc_conn.commit()
            logger.info("Resultados de clustering guardados exitosamente")
            
        except Exception as e:
            self.proc_conn.rollback()
            logger.error(f"Error guardando resultados de clustering: {e}")
            raise
        finally:
            cursor.close()
    
    def run_full_clustering(self) -> Dict:
        """
        Ejecutar pipeline completo de clustering
        
        Returns:
            Dict con todos los resultados
        """
        logger.info("Iniciando pipeline completo de clustering...")
        
        try:
            self.connect_processor_db()
            
            # Cargar datos
            df = self.load_pois_data()
            
            if df.empty:
                logger.warning("No hay datos para clustering")
                return {'status': 'no_data', 'message': 'Sin datos para procesar'}
            
            # üöÄ Ejecutar SOLO algoritmos ML usados en producci√≥n (OPTIMIZADO)
            results = {}
            
            logger.info("ü§ñ Ejecutando algoritmos ML activos...")
            
            # ‚úÖ 1. Clustering geogr√°fico (K-means) - USADO para perfiles de usuario
            results['geographic'] = self.geographic_clustering(df)
            
            # ‚úÖ 2. Clustering DBSCAN - USADO para optimizaci√≥n de rutas geogr√°ficas
            results['dbscan'] = self.dbscan_clustering(df)
            
            # ‚úÖ 3. Clustering jer√°rquico - USADO para relaciones entre categor√≠as
            results['hierarchical'] = self.hierarchical_clustering(df)
            
            # üóëÔ∏è ALGORITMOS ELIMINADOS (no se usaban en recommendation_service):
            # ‚ùå results['category'] = self.category_clustering(df)  # Redundante con jer√°rquico
            # ‚ùå results['neighborhood'] = self.neighborhood_clustering(df)  # No usado en filtrado
            # ‚ùå results['tourist_zones'] = self.detect_tourist_zones()  # No implementado en recomendaciones
            
            # Guardar resultados
            self.save_clustering_results(results)
            
            # Resumen general
            successful_algorithms = []
            for alg_name, result in results.items():
                if isinstance(result, dict) and result.get('status') == 'success':
                    successful_algorithms.append(alg_name)
            
            results['summary'] = {
                'total_pois_processed': len(df),
                'algorithms_executed': len(successful_algorithms),
                'successful_algorithms': successful_algorithms,
                'optimization_applied': 'Solo algoritmos ML usados en producci√≥n',
                'algorithms_eliminated': ['category', 'neighborhood', 'tourist_zones'],
                'performance_improvement': f'Reducci√≥n ~{((6-3)/6)*100:.0f}% tiempo de procesamiento',
                'best_silhouette_score': max([
                    results['geographic'].get('silhouette_score', 0),
                    results['dbscan'].get('silhouette_score', 0), 
                    results['hierarchical'].get('silhouette_score', 0)
                ]),
                'execution_time': datetime.now().isoformat()
            }
            
            logger.info("üöÄ Pipeline de clustering OPTIMIZADO completado exitosamente")
            logger.info(f"üìä Algoritmos ML activos: {successful_algorithms}")
            results['status'] = 'success'  # Agregar status de √©xito
            return results
            
        except Exception as e:
            logger.error(f"Error en pipeline de clustering: {e}")
            return {'status': 'error', 'message': str(e)}
        finally:
            self.disconnect_processor_db()
    
    def _convert_numpy_types(self, obj):
        """Convertir tipos numpy a tipos nativos de Python para JSON/BD"""
        import json
        
        if isinstance(obj, dict):
            return {key: self._convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_numpy_types(item) for item in obj]
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj

    def _calculate_category_relationships(self, df_clustered: pd.DataFrame) -> Dict:
        """
        Calcular relaciones entre categor√≠as bas√°ndose en co-ocurrencia en clusters
        
        Args:
            df_clustered: DataFrame con columna cluster_hierarchical y categoria
            
        Returns:
            Dict con relaciones: {categoria: {categoria_relacionada: score}}
        """
        relationships = {}
        
        # Obtener categor√≠as √∫nicas
        categorias = df_clustered['categoria'].unique()
        
        for cat in categorias:
            relationships[cat] = {}
            
            # Encontrar en qu√© clusters aparece esta categor√≠a
            pois_cat = df_clustered[df_clustered['categoria'] == cat]
            clusters_cat = pois_cat['cluster_hierarchical'].value_counts()
            
            # Para cada cluster donde aparece esta categor√≠a
            for cluster_id in clusters_cat.index:
                # Encontrar otras categor√≠as en el mismo cluster
                pois_en_cluster = df_clustered[df_clustered['cluster_hierarchical'] == cluster_id]
                otras_categorias = pois_en_cluster[pois_en_cluster['categoria'] != cat]['categoria'].value_counts()
                
                # Calcular score de relaci√≥n basado en co-ocurrencia
                for otra_cat, count in otras_categorias.items():
                    if otra_cat not in relationships[cat]:
                        relationships[cat][otra_cat] = 0.0
                    
                    # Score basado en proporci√≥n de POIs compartidos en el cluster
                    total_en_cluster = len(pois_en_cluster)
                    score = count / total_en_cluster if total_en_cluster > 0 else 0.0
                    relationships[cat][otra_cat] = max(relationships[cat][otra_cat], score)
        
        return relationships
    
    # M√©todos auxiliares
    def _calculate_cluster_radius(self, cluster_data: pd.DataFrame) -> float:
        """Calcular radio del cluster en km"""
        if len(cluster_data) < 2:
            return 0.0
        
        coords = cluster_data[['latitud', 'longitud']].astype(float).values
        center = coords.mean(axis=0)
        
        # Calcular distancia m√°xima al centro (aproximaci√≥n simple)
        distances = np.sqrt(np.sum((coords - center) ** 2, axis=1))
        max_distance = np.max(distances)
        
        # Convertir de grados a km (aproximado)
        return float(max_distance * 111)  # 1 grado ‚âà 111 km
    
    def _calculate_geographic_density(self, data: pd.DataFrame) -> float:
        """Calcular densidad geogr√°fica de POIs"""
        if len(data) < 2:
            return 0.0
        
        coords = data[['latitud', 'longitud']].astype(float).values
        lat_range = coords[:, 0].max() - coords[:, 0].min()
        lng_range = coords[:, 1].max() - coords[:, 1].min()
        
        if lat_range == 0 or lng_range == 0:
            return 0.0
        
        # √Årea aproximada en km¬≤
        area_km2 = lat_range * lng_range * 111 * 111
        return len(data) / area_km2 if area_km2 > 0 else 0.0
    
    def _estimate_poi_density(self, barrio_data: pd.DataFrame) -> float:
        """Estimar densidad de POIs por km¬≤ en un barrio"""
        # Estimaci√≥n simple basada en dispersi√≥n de coordenadas
        if len(barrio_data) < 2:
            return 0.0
        
        lat_std = barrio_data['latitud'].astype(float).std()
        lng_std = barrio_data['longitud'].astype(float).std()
        
        # √Årea estimada (muy aproximada)
        area_estimate = lat_std * lng_std * 111 * 111 * 4  # 4 = factor de √°rea el√≠ptica
        
        return len(barrio_data) / area_estimate if area_estimate > 0 else 0.0
    
    def _get_best_poi(self, data: pd.DataFrame) -> Dict:
        """Obtener el POI mejor valorado de un grupo"""
        if len(data) == 0:
            return {}
        
        # Filtrar POIs con valoraci√≥n
        rated_pois = data[data['valoracion_promedio'] > 0]
        
        if len(rated_pois) == 0:
            # Si no hay valoraciones, tomar el primero
            best_poi = data.iloc[0]
        else:
            # Tomar el mejor valorado
            best_poi = rated_pois.loc[rated_pois['valoracion_promedio'].idxmax()]
        
        return {
            'nombre': best_poi['nombre'],
            'categoria': best_poi['categoria'],
            'valoracion': float(best_poi['valoracion_promedio']) if pd.notna(best_poi['valoracion_promedio']) else 0.0
        }
    
    def _calculate_neighborhood_rankings(self, neighborhood_analysis: Dict) -> Dict:
        """Calcular rankings de barrios por diferentes m√©tricas"""
        if not neighborhood_analysis:
            return {}
        
        # Ranking por densidad de POIs
        density_ranking = sorted(
            neighborhood_analysis.items(),
            key=lambda x: x[1]['densidad_poi_km2'],
            reverse=True
        )[:10]
        
        # Ranking por valoraci√≥n promedio
        rating_ranking = sorted(
            neighborhood_analysis.items(),
            key=lambda x: x[1]['valoracion_promedio'],
            reverse=True
        )[:10]
        
        # Ranking por diversidad de categor√≠as
        diversity_ranking = sorted(
            neighborhood_analysis.items(),
            key=lambda x: x[1]['diversidad_categoria'],
            reverse=True
        )[:10]
        
        return {
            'top_density': [{'barrio': name, 'valor': data['densidad_poi_km2']} for name, data in density_ranking],
            'top_rating': [{'barrio': name, 'valor': data['valoracion_promedio']} for name, data in rating_ranking],
            'top_diversity': [{'barrio': name, 'valor': data['diversidad_categoria']} for name, data in diversity_ranking]
        }
    
    def _generate_zone_description(self, cluster: Dict) -> str:
        """Generar descripci√≥n textual de una zona tur√≠stica"""
        main_categories = list(cluster['categorias'].keys())[:2]
        main_neighborhoods = list(cluster['barrios'].keys())[:2]
        
        desc = f"Zona tur√≠stica con {cluster['num_pois']} POIs"
        
        if main_categories:
            desc += f", principalmente {' y '.join(main_categories).lower()}"
        
        if main_neighborhoods:
            desc += f", ubicada en {' y '.join(main_neighborhoods)}"
        
        return desc
