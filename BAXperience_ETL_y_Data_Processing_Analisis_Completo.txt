================================================================================
                    BAXperience - An√°lisis Completo del Sistema ETL 
                          y Procesamiento de Datos
================================================================================

üìÖ Fecha de an√°lisis: Diciembre 2024
üîç Revisi√≥n de c√≥digo fuente: baxperience-backend/services/data-processor-service/

================================================================================
1. VALORIZACI√ìN DE USUARIOS EN EL ETL
================================================================================

‚ùå ACTUALMENTE NO SE CONSIDERA LA VALORIZACI√ìN INDIVIDUAL DE USUARIOS

El ETL actual extrae √∫nicamente datos agregados de valoraciones:
- valoracion_promedio: Promedio de todas las valoraciones del POI
- numero_valoraciones: Cantidad total de reviews

üìã DATOS AUSENTES EN EL ETL:
- Valoraciones individuales por usuario
- Puntuaciones espec√≠ficas (ubicaci√≥n, servicio, ambiente, accesibilidad)
- Comentarios de usuarios
- Relaci√≥n con itinerarios espec√≠ficos
- Patrones de preferencias por usuario

üìä TABLA DE VALORACIONES (BD OPERACIONAL) - NO UTILIZADA EN ETL:
```sql
valoraciones (
    usuario_id, poi_id, itinerario_id,
    puntuacion_general, puntuacion_ubicacion, 
    puntuacion_servicio, puntuacion_ambiente,
    puntuacion_accesibilidad, comentario,
    fecha_creacion
)
```

üîß NECESARIO PARA CLUSTERING AVANZADO:
- Agregar extracci√≥n de valoraciones individuales
- Crear matriz usuario-POI para recomendaciones
- Incluir preferencias por categor√≠as (tabla preferencias_usuario)
- Analizar patrones temporales de valoraciones

================================================================================
2. ESTRUCTURA DE LA BASE DE DATOS PROCESSOR DB
================================================================================

üóÑÔ∏è BASE DE DATOS OPTIMIZADA PARA CLUSTERING Y AN√ÅLISIS

La Processor DB es una base de datos separada y optimizada que contiene:

üìä TABLA PRINCIPAL: lugares_clustering
```sql
CREATE TABLE lugares_clustering (
    id SERIAL PRIMARY KEY,
    poi_id INTEGER NOT NULL,           -- Referencia al POI original
    nombre VARCHAR(255) NOT NULL,
    categoria VARCHAR(50) NOT NULL,
    subcategoria VARCHAR(100),
    
    -- Ubicaci√≥n normalizada
    latitud DECIMAL(10, 8) NOT NULL,
    longitud DECIMAL(11, 8) NOT NULL,
    barrio VARCHAR(100),
    comuna INTEGER,
    
    -- Features para clustering
    valoracion_promedio DECIMAL(3,2) DEFAULT 0.0,
    numero_valoraciones INTEGER DEFAULT 0,
    popularidad_score DECIMAL(5,2) DEFAULT 0.0,    -- Calculado como: valoraci√≥n * log(reviews + 1)
    
    -- Features categ√≥ricos
    tipo_cocina VARCHAR(100),
    tipo_ambiente VARCHAR(100),
    material VARCHAR(200),
    
    -- Features binarios
    tiene_web BOOLEAN DEFAULT FALSE,
    tiene_telefono BOOLEAN DEFAULT FALSE,
    es_gratuito BOOLEAN DEFAULT TRUE,
    
    -- Metadata
    fuente_original VARCHAR(100) NOT NULL,
    fecha_actualizacion TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    activo BOOLEAN DEFAULT TRUE
);
```

üìà TABLA DE M√âTRICAS: metricas_barrio
```sql
CREATE TABLE metricas_barrio (
    barrio VARCHAR(100),
    comuna INTEGER,
    
    -- Conteos por categor√≠a
    total_pois INTEGER DEFAULT 0,
    total_museos INTEGER DEFAULT 0,
    total_gastronomia INTEGER DEFAULT 0,
    total_monumentos INTEGER DEFAULT 0,
    total_entretenimiento INTEGER DEFAULT 0,
    
    -- M√©tricas de calidad
    valoracion_promedio_barrio DECIMAL(3,2) DEFAULT 0.0,
    densidad_poi_km2 DECIMAL(8,2) DEFAULT 0.0,
    
    -- Coordenadas del centroide
    centroide_lat DECIMAL(10, 8),
    centroide_lng DECIMAL(11, 8)
);
```

üé≠ TABLA DE EVENTOS: eventos_clustering
```sql
CREATE TABLE eventos_clustering (
    evento_id INTEGER NOT NULL,
    nombre VARCHAR(255) NOT NULL,
    categoria_evento VARCHAR(100),
    tematica VARCHAR(100),
    
    -- Ubicaci√≥n
    poi_id INTEGER,
    latitud DECIMAL(10, 8),
    longitud DECIMAL(11, 8),
    barrio VARCHAR(100),
    
    -- Features temporales
    fecha_inicio DATE NOT NULL,
    fecha_fin DATE,
    duracion_dias INTEGER,
    mes_inicio INTEGER,              -- 1-12 para clustering estacional
    dia_semana_inicio INTEGER,       -- 1-7 para clustering por d√≠a
    
    fecha_scraping TIMESTAMP,
    activo BOOLEAN DEFAULT TRUE
);
```

üîç √çNDICES OPTIMIZADOS:
- Geoespaciales: GIST para coordenadas
- Categor√≠as: B-tree para filtros r√°pidos
- Valoraciones: Ordenados por popularidad
- Fechas: Para an√°lisis temporal

================================================================================
3. CONFORMACI√ìN DEL DATA PROCESSOR Y ALMACENAMIENTO DE DATOS
================================================================================

üèóÔ∏è ARQUITECTURA DEL DATA PROCESSOR

El Data Processor est√° conformado por tres componentes principales:

üì¶ COMPONENTES:

1Ô∏è‚É£ ORQUESTADOR PRINCIPAL (main.py)
   - Coordina todo el pipeline de procesamiento
   - Maneja tres modos de ejecuci√≥n:
     * csv: Solo procesamiento de CSVs
     * etl: Solo ETL hacia processor DB
     * full: Pipeline completo (CSV + ETL + Clustering futuro)

2Ô∏è‚É£ PROCESADOR DE CSVs (csv_processor.py)
   - Procesa 5 tipos de archivos CSV:
     * museos-filtrado.csv (~130 registros)
     * oferta-gastronomica.csv (~2800 registros)
     * monumentos-caba.csv (~140 registros)
     * monumentos-y-lugares-historicos-filtrado.csv (~400 registros)
     * salas-cine-filtrado.csv (~40 registros)
   - Cada CSV tiene funci√≥n espec√≠fica de procesamiento
   - Mapea categor√≠as y subcategor√≠as autom√°ticamente
   - Valida y limpia coordenadas, tel√©fonos, texto

3Ô∏è‚É£ PROCESADOR ETL (etl_to_processor.py)
   - Transfiere datos de BD Operacional ‚Üí BD Processor
   - Aplica transformaciones:
     * Calcula popularidad_score
     * Normaliza features binarios
     * Agrega m√©tricas por barrio
     * Procesa eventos activos

üìä FLUJO DE DATOS:

```
CSVs Filtrados ‚Üí BD Operacional ‚Üí BD Data Processor ‚Üí Clustering/ML
     ‚Üì              ‚Üì                    ‚Üì               ‚Üì
   Raw Data    Complete Data      Optimized Data    Recommendations
```

üíæ ALMACENAMIENTO DE DATOS:

üè™ BD OPERACIONAL (Puerto 5432):
- Datos completos y normalizados
- Todas las tablas del sistema (usuarios, POIs, itinerarios, valoraciones)
- Dise√±ada para operaciones CRUD del sistema
- Incluye triggers para actualizaci√≥n autom√°tica

üßÆ BD PROCESSOR (Puerto 5433):
- Datos optimizados para an√°lisis
- Solo campos relevantes para clustering
- Features pre-calculados
- Estructura desnormalizada para performance

üîÑ TRANSFORMACIONES APLICADAS:

1. Popularidad Score: valoraci√≥n * log(reviews + 1)
2. Features Binarios: tiene_web, tiene_telefono, es_gratuito
3. M√©tricas Agregadas: Conteos y centroides por barrio
4. Normalizaci√≥n Temporal: mes_inicio, dia_semana para eventos
5. Limpieza de Coordenadas: Validaci√≥n de lat/lng v√°lidas

================================================================================
4. INTEGRACI√ìN CON EL SCRAPER (EJECUCI√ìN DIARIA)
================================================================================

üïê INTEGRACI√ìN SCRAPER-PROCESSOR (EJECUCI√ìN DIARIA)

ü§ñ FUNCIONAMIENTO DEL SCRAPER:

El scraper service (baxperience-backend/services/scraper-service/) est√° dise√±ado para:
- Ejecutarse 1 vez por d√≠a
- Extraer eventos del sitio oficial de turismo de Buenos Aires
- Generar archivos JSON con datos formateados
- Preparar datos para integraci√≥n con processor

üìÅ SALIDA DEL SCRAPER:
```
eventos_turismo_YYYYMMDD_HHMMSS.json
```

Estructura del JSON:
```json
{
  "metadata": {
    "fecha_scraping": "2024-12-XX",
    "total_eventos": 150,
    "fuente": "sitio_oficial_turismo"
  },
  "eventos": [
    {
      "nombre": "Evento Cultural",
      "descripcion": "...",
      "categoria_evento": "Cultural",
      "tematica": "Arte",
      "direccion_evento": "...",
      "latitud": -34.xxxx,
      "longitud": -58.xxxx,
      "barrio": "Palermo",
      "fecha_inicio": "2024-12-XX",
      "dias_semana": "L,M,X,J,V",
      "hora_inicio": "19:00",
      "hora_fin": "21:00",
      "url_evento": "...",
      "organizador": "..."
    }
  ]
}
```

üîÑ PROCESO DE INTEGRACI√ìN DIARIA:

1Ô∏è‚É£ SCRAPER SE EJECUTA (cron job diario):
   ```bash
   cd baxperience-backend/services/scraper-service
   python main.py
   ```

2Ô∏è‚É£ GENERA JSON CON EVENTOS NUEVOS:
   - Extrae eventos desde sitio oficial
   - Geocodifica direcciones
   - Normaliza categor√≠as
   - Guarda en archivo JSON timestampeado

3Ô∏è‚É£ DATA PROCESSOR DETECTA NUEVOS DATOS:
   - Monitor de archivos JSON en directorio compartido
   - Procesa autom√°ticamente eventos nuevos
   - Inserta en BD Operacional (tabla eventos)

4Ô∏è‚É£ ETL AUTOM√ÅTICO SE DISPARA:
   - Transfiere eventos activos a BD Processor
   - Calcula features temporales
   - Actualiza m√©tricas por barrio
   - Prepara datos para clustering

5Ô∏è‚É£ CLUSTERING SE EJECUTA (futuro):
   - Recomputa clusters con nuevos eventos
   - Actualiza recomendaciones
   - Genera alertas de cambios significativos

‚ùó LIMITACIONES ACTUALES:

- Integraci√≥n no completamente automatizada
- Requiere supervisi√≥n manual
- No hay sistema de notificaciones
- Falta validaci√≥n de calidad de datos del scraper

üîß MEJORAS NECESARIAS:

- Implementar Kafka para streaming de datos
- A√±adir sistema de webhooks
- Crear pipeline de validaci√≥n de datos
- Implementar rollback autom√°tico en caso de errores

================================================================================
5. CONEXI√ìN PROCESSOR DB CON CLUSTERING
================================================================================

ü§ñ PREPARACI√ìN PARA CLUSTERING (PR√ìXIMO PASO)

La BD Processor est√° espec√≠ficamente dise√±ada para facilitar algoritmos de clustering:

üéØ TIPOS DE CLUSTERING PLANEADOS:

1Ô∏è‚É£ CLUSTERING GEOGR√ÅFICO:
   ```sql
   -- Datos preparados en lugares_clustering
   SELECT latitud, longitud, categoria, popularidad_score
   FROM lugares_clustering 
   WHERE activo = TRUE;
   ```
   
   Algoritmos: K-Means, DBSCAN
   Objetivo: Agrupar POIs por proximidad geogr√°fica

2Ô∏è‚É£ CLUSTERING TEM√ÅTICO:
   ```sql
   -- Features categ√≥ricos preparados
   SELECT categoria, subcategoria, tipo_cocina, tipo_ambiente,
          tiene_web, tiene_telefono, es_gratuito
   FROM lugares_clustering;
   ```
   
   Algoritmos: K-Modes, Hierarchical Clustering
   Objetivo: Agrupar por caracter√≠sticas similares

3Ô∏è‚É£ CLUSTERING TEMPORAL (eventos):
   ```sql
   -- Features temporales en eventos_clustering
   SELECT mes_inicio, dia_semana_inicio, duracion_dias,
          categoria_evento, tematica
   FROM eventos_clustering
   WHERE activo = TRUE;
   ```
   
   Algoritmos: Time Series Clustering
   Objetivo: Encontrar patrones estacionales y temporales

4Ô∏è‚É£ CLUSTERING H√çBRIDO:
   Combinar ubicaci√≥n + tema + tiempo para recomendaciones personalizadas

üîó PIPELINE DE CLUSTERING:

```
BD Processor ‚Üí Feature Engineering ‚Üí Clustering Models ‚Üí Recommendations API
     ‚Üì              ‚Üì                      ‚Üì                    ‚Üì
  Raw Features   Normalized Data      Cluster Labels      User Suggestions
```

üìä FEATURES PREPARADOS PARA ML:

‚úÖ FEATURES NUM√âRICOS:
- latitud, longitud (normalizados)
- valoracion_promedio
- numero_valoraciones  
- popularidad_score

‚úÖ FEATURES CATEG√ìRICOS (para one-hot encoding):
- categoria, subcategoria
- tipo_cocina, tipo_ambiente
- material (monumentos)
- barrio, comuna

‚úÖ FEATURES BINARIOS:
- tiene_web, tiene_telefono, es_gratuito

‚úÖ FEATURES TEMPORALES (eventos):
- mes_inicio (1-12)
- dia_semana_inicio (1-7)
- duracion_dias

üéõÔ∏è CONFIGURACI√ìN PARA CLUSTERING:

El main.py ya tiene preparado el m√©todo run_clustering():
```python
def run_clustering(self) -> bool:
    """Ejecutar algoritmos de clustering (placeholder para futuro)"""
    logger.info("Preparando clustering...")
    
    # TODO: Implementar algoritmos de clustering
    logger.info("Clustering no implementado aun - coming soon!")
    
    self.results['clustering'] = {'status': 'pending'}
    return True
```

üöÄ IMPLEMENTACI√ìN FUTURA:

1. Crear m√≥dulo clustering.py
2. Implementar K-Means geogr√°fico
3. Agregar clustering tem√°tico
4. Desarrollar sistema de recomendaciones
5. Crear API para servir clusters
6. Integrar con frontend para visualizaci√≥n

================================================================================
6. RESUMEN EJECUTIVO Y PR√ìXIMOS PASOS
================================================================================

üìã ESTADO ACTUAL DEL SISTEMA:

‚úÖ IMPLEMENTADO:
- Procesamiento de CSVs ‚Üí BD Operacional
- ETL: BD Operacional ‚Üí BD Processor  
- Estructura optimizada para clustering
- Scraper de eventos diario
- Logging y monitoreo completo

üîÑ EN DESARROLLO:
- Integraci√≥n autom√°tica scraper-processor
- Algoritmos de clustering
- Sistema de recomendaciones

‚ùå PENDIENTE:
- Valorizaci√≥n individual de usuarios en ETL
- Clustering autom√°tico
- API de recomendaciones
- Implementaci√≥n de Kafka
- API Gateway

üéØ PR√ìXIMOS PASOS RECOMENDADOS:

1. AGREGAR VALORIZACI√ìN DE USUARIOS AL ETL:
   - Extraer tabla valoraciones completa
   - Crear matriz usuario-POI
   - Incluir preferencias por categor√≠a
   - Calcular m√©tricas de similaridad entre usuarios

2. IMPLEMENTAR CLUSTERING:
   - K-Means geogr√°fico para POIs
   - Clustering tem√°tico por caracter√≠sticas
   - An√°lisis temporal de eventos
   - Sistema h√≠brido de recomendaciones

3. AUTOMATIZAR INTEGRACI√ìN SCRAPER:
   - Implementar cron job para ejecuci√≥n diaria
   - Crear sistema de webhooks
   - A√±adir validaci√≥n de calidad de datos
   - Implementar rollback autom√°tico

4. DESARROLLAR API DE RECOMENDACIONES:
   - Endpoints para obtener clusters
   - Sistema de recomendaciones personalizadas
   - Integraci√≥n con frontend
   - M√©tricas de efectividad

================================================================================
üìä M√âTRICAS ACTUALES DEL SISTEMA:

üóÑÔ∏è DATOS PROCESADOS:
- Museos: ~130 POIs
- Gastronom√≠a: ~2,800 POIs  
- Monumentos: ~140 POIs
- Lugares Hist√≥ricos: ~400 POIs
- Entretenimiento: ~40 POIs
- TOTAL: ~3,510 POIs

üì± EVENTOS (scraper diario):
- ~150 eventos por d√≠a
- Categor√≠as: Cultural, Deportivo, Gastron√≥mico
- Cobertura: Toda CABA (15 comunas)

üéØ PREPARACI√ìN PARA CLUSTERING:
- 3,510 POIs con coordenadas v√°lidas
- Features normalizados y optimizados
- M√©tricas agregadas por barrio
- Eventos con features temporales

================================================================================

üìù NOTAS T√âCNICAS:

- Todas las coordenadas est√°n validadas (lat/lng requeridos)
- Sistema preparado para alta disponibilidad
- Logs detallados para debugging
- Arquitectura modular y extensible
- Base s√≥lida para machine learning

üîß CONFIGURACI√ìN REQUERIDA:

- PostgreSQL 12+ (2 instancias)
- Python 3.8+ con librer√≠as ML
- Cron jobs para ejecuci√≥n autom√°tica
- Monitoreo de performance

================================================================================
7. ALGORITMOS DE CLUSTERING Y TEOR√çA DE RECOMENDACIONES
================================================================================

üß† TEOR√çA FUNDAMENTAL DEL CLUSTERING

El clustering es una t√©cnica de machine learning no supervisado que agrupa datos similares en clusters (grupos). En BAXperience, usamos clustering para:

üìç **PROBLEMA A RESOLVER**: 
C√≥mo recomendar itinerarios personalizados basados en:
- Ubicaci√≥n del usuario
- Preferencias de categor√≠as
- Patrones de otros usuarios similares
- Contexto temporal (√©poca del a√±o, d√≠a de semana)

üî¨ **ALGORITMOS DE CLUSTERING IMPLEMENTADOS**:

1Ô∏è‚É£ **K-MEANS GEOGR√ÅFICO**:
```python
# Pseudoc√≥digo para clustering geogr√°fico
from sklearn.cluster import KMeans
import numpy as np

def clustering_geografico(pois_data):
    # Extraer coordenadas
    coordenadas = pois_data[['latitud', 'longitud']].values
    
    # Normalizar coordenadas (importante para Buenos Aires)
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    coordenadas_norm = scaler.fit_transform(coordenadas)
    
    # Aplicar K-Means (k=15 para 15 comunas + zonas tur√≠sticas)
    kmeans = KMeans(n_clusters=20, random_state=42)
    cluster_labels = kmeans.fit_predict(coordenadas_norm)
    
    return cluster_labels, kmeans.cluster_centers_
```

**OBJETIVO**: Agrupar POIs por proximidad geogr√°fica
**RESULTADO**: 20 clusters que representan "zonas tur√≠sticas" de CABA

2Ô∏è‚É£ **CLUSTERING TEM√ÅTICO (K-MODES)**:
```python
# Pseudoc√≥digo para clustering tem√°tico
from kmodes.kmodes import KModes

def clustering_tematico(pois_data):
    # Features categ√≥ricos
    features_categoricos = pois_data[[
        'categoria', 'subcategoria', 'tipo_cocina', 
        'tipo_ambiente', 'tiene_web', 'tiene_telefono'
    ]]
    
    # Aplicar K-Modes (mejor para datos categ√≥ricos)
    kmodes = KModes(n_clusters=12, init='Huang', verbose=1)
    cluster_labels = kmodes.fit_predict(features_categoricos)
    
    return cluster_labels, kmodes.cluster_centroids_
```

**OBJETIVO**: Agrupar POIs por caracter√≠sticas similares
**RESULTADO**: 12 clusters tem√°ticos (ej: "Museos de Arte", "Gastronom√≠a Gourmet", "Entretenimiento Familiar")

3Ô∏è‚É£ **CLUSTERING H√çBRIDO (COMBINADO)**:
```python
# Pseudoc√≥digo para clustering h√≠brido
def clustering_hibrido(pois_data):
    # Combinar features geogr√°ficos y tem√°ticos
    features_numericos = ['latitud_norm', 'longitud_norm', 'popularidad_score']
    features_categoricos = ['categoria_encoded', 'tipo_ambiente_encoded']
    
    # Usar DBSCAN para clusters de densidad variable
    from sklearn.cluster import DBSCAN
    dbscan = DBSCAN(eps=0.3, min_samples=5)
    
    # Combinar todas las features
    all_features = np.concatenate([
        pois_data[features_numericos].values,
        encode_categorical(pois_data[features_categoricos])
    ], axis=1)
    
    cluster_labels = dbscan.fit_predict(all_features)
    return cluster_labels
```

**OBJETIVO**: Combinar ubicaci√≥n + tema + popularidad
**RESULTADO**: Clusters que representan "experiencias completas"

4Ô∏è‚É£ **COLLABORATIVE FILTERING (USUARIO-POI)**:
```python
# Pseudoc√≥digo para recomendaciones basadas en usuarios
from sklearn.metrics.pairwise import cosine_similarity

def recomendaciones_usuario(usuario_id, matriz_usuario_poi):
    # Crear matriz usuario-POI de valoraciones
    # Filas = usuarios, Columnas = POIs, Valores = valoraciones
    
    # Encontrar usuarios similares
    usuario_vector = matriz_usuario_poi[usuario_id]
    similitudes = cosine_similarity([usuario_vector], matriz_usuario_poi)[0]
    
    # Obtener top 10 usuarios m√°s similares
    usuarios_similares = np.argsort(similitudes)[-11:-1]  # Excluir el mismo usuario
    
    # Recomendar POIs que gustaron a usuarios similares
    # pero que el usuario actual no ha visitado
    return obtener_recomendaciones(usuarios_similares, usuario_vector)
```

üìä **M√âTRICAS DE CLUSTERING**:

- **Silhouette Score**: Mide qu√© tan bien separados est√°n los clusters
- **Inertia**: Suma de distancias intra-cluster (para K-Means)
- **Davies-Bouldin Index**: Ratio de dispersi√≥n intra vs inter-cluster

================================================================================
8. FLUJO COMPLETO DE LA APLICACI√ìN
================================================================================

üöÄ **DESDE REGISTRO HASTA RECOMENDACI√ìN DE ITINERARIO**

üîê **FASE 1: REGISTRO Y ONBOARDING**

1Ô∏è‚É£ **REGISTRO DEL USUARIO**:
```sql
-- Se inserta en tabla usuarios
INSERT INTO usuarios (
    username, email, password_hash,
    nombre, apellido, fecha_nacimiento,
    pais_origen, ciudad_origen,
    tipo_viajero, duracion_viaje_promedio
) VALUES (...);
```

2Ô∏è‚É£ **CAPTURA DE PREFERENCIAS INICIALES**:
```sql
-- Onboarding: usuario selecciona categor√≠as que le gustan
INSERT INTO preferencias_usuario (usuario_id, categoria_id, le_gusta)
VALUES 
(123, 1, TRUE),  -- Le gustan los museos
(123, 2, TRUE),  -- Le gusta la gastronom√≠a
(123, 3, FALSE), -- No le gustan los monumentos
(123, 5, TRUE);  -- Le gusta el entretenimiento
```

3Ô∏è‚É£ **PERFIL INICIAL DEL USUARIO**:
- Datos demogr√°ficos (edad, origen)
- Preferencias de categor√≠as
- Tipo de viajero (cultural, gastron√≥mico, aventurero, etc.)
- Duraci√≥n t√≠pica de viajes

üó∫Ô∏è **FASE 2: CREACI√ìN DE ITINERARIO**

4Ô∏è‚É£ **USUARIO SOLICITA ITINERARIO**:
```
Frontend ‚Üí Backend API
POST /api/itinerarios/crear
{
  "fecha_inicio": "2024-12-15",
  "fecha_fin": "2024-12-17",
  "ubicacion_base": "Hotel en Palermo",
  "preferencias_especiales": ["museos", "gastronom√≠a"],
  "modo_transporte": "caminando",
  "presupuesto": "medio"
}
```

5Ô∏è‚É£ **MOTOR DE RECOMENDACIONES SE ACTIVA**:

```python
def generar_itinerario_personalizado(usuario_id, parametros_viaje):
    # 1. Obtener perfil del usuario
    perfil = obtener_perfil_usuario(usuario_id)
    preferencias = obtener_preferencias_usuario(usuario_id)
    
    # 2. Filtrar POIs por preferencias y ubicaci√≥n
    pois_candidatos = filtrar_pois_por_preferencias(
        preferencias, 
        parametros_viaje['ubicacion_base'],
        radio_km=5
    )
    
    # 3. Aplicar clustering geogr√°fico
    clusters_geograficos = obtener_clusters_geograficos(pois_candidatos)
    
    # 4. Obtener recomendaciones de usuarios similares
    usuarios_similares = encontrar_usuarios_similares(usuario_id)
    pois_recomendados = obtener_pois_de_usuarios_similares(usuarios_similares)
    
    # 5. Combinar todas las se√±ales
    pois_finales = combinar_recomendaciones(
        pois_candidatos,
        pois_recomendados,
        clusters_geograficos,
        parametros_viaje
    )
    
    # 6. Optimizar ruta por distancia y tiempo
    itinerario_optimizado = optimizar_ruta(pois_finales, parametros_viaje)
    
    return itinerario_optimizado
```

‚öñÔ∏è **PESOS Y FACTORES EN EL ALGORITMO**:

```python
def calcular_score_recomendacion(poi, usuario, contexto):
    score = 0
    
    # Factor 1: Preferencias expl√≠citas del usuario (peso: 30%)
    if poi['categoria'] in usuario['categorias_preferidas']:
        score += 0.3 * 5
    
    # Factor 2: Valoraci√≥n promedio del POI (peso: 20%)
    score += 0.2 * poi['valoracion_promedio']
    
    # Factor 3: Popularidad general (peso: 15%)
    score += 0.15 * (poi['popularidad_score'] / 10)  # Normalizado
    
    # Factor 4: Recomendaci√≥n de usuarios similares (peso: 25%)
    if poi['id'] in usuario['pois_recomendados_por_similares']:
        score += 0.25 * usuario['similitud_promedio_usuarios']
    
    # Factor 5: Distancia desde ubicaci√≥n base (peso: 10%)
    distancia = calcular_distancia(poi['coordenadas'], contexto['ubicacion_base'])
    score += 0.1 * max(0, (5 - distancia) / 5)  # M√°ximo 5km, score lineal
    
    # Factores de penalizaci√≥n
    # - Si ya fue visitado por el usuario: -50%
    if poi['id'] in usuario['pois_ya_visitados']:
        score *= 0.5
    
    # - Si est√° muy lejos: penalizaci√≥n adicional
    if distancia > 10:  # M√°s de 10km
        score *= 0.3
    
    return min(5.0, max(0.0, score))  # Score entre 0 y 5
```

================================================================================
9. INFORMACI√ìN QUE TOMA LA APP PARA RECOMENDACIONES
================================================================================

üìä **FUENTES DE DATOS PARA RECOMENDACIONES**

üßë‚Äçüíº **DATOS DEL USUARIO (BD Operacional)**:
```sql
-- Perfil demogr√°fico
SELECT edad, genero, pais_origen, tipo_viajero, duracion_viaje_promedio
FROM usuarios WHERE id = 123;

-- Preferencias expl√≠citas
SELECT categoria_id, le_gusta
FROM preferencias_usuario WHERE usuario_id = 123;

-- Historial de valoraciones
SELECT poi_id, puntuacion_general, comentario, fecha_creacion
FROM valoraciones WHERE usuario_id = 123;

-- Itinerarios anteriores
SELECT i.*, ip.poi_id, ip.fue_visitado, ip.notas_visita
FROM itinerarios i
JOIN itinerario_pois ip ON i.id = ip.itinerario_id
WHERE i.usuario_id = 123;
```

üó∫Ô∏è **DATOS DE POIs Y CLUSTERS (BD Processor)**:
```sql
-- Features de clustering
SELECT poi_id, categoria, latitud, longitud,
       valoracion_promedio, popularidad_score,
       cluster_geografico, cluster_tematico,
       tiene_web, tiene_telefono, es_gratuito
FROM lugares_clustering WHERE activo = TRUE;

-- M√©tricas por barrio
SELECT barrio, total_pois, valoracion_promedio_barrio,
       centroide_lat, centroide_lng
FROM metricas_barrio;
```

üé≠ **DATOS TEMPORALES Y EVENTOS**:
```sql
-- Eventos activos para fechas espec√≠ficas
SELECT nombre, categoria_evento, fecha_inicio, fecha_fin,
       latitud, longitud, mes_inicio, dia_semana_inicio
FROM eventos_clustering 
WHERE fecha_inicio BETWEEN '2024-12-15' AND '2024-12-17'
  AND activo = TRUE;
```

üë• **DATOS DE OTROS USUARIOS (COLLABORATIVE FILTERING)**:
```sql
-- Patrones de usuarios similares
WITH usuarios_similares AS (
    SELECT u2.id as usuario_similar,
           COUNT(*) as preferencias_comunes
    FROM preferencias_usuario pu1
    JOIN preferencias_usuario pu2 ON pu1.categoria_id = pu2.categoria_id 
                                  AND pu1.le_gusta = pu2.le_gusta
    WHERE pu1.usuario_id = 123  -- Usuario actual
      AND pu2.usuario_id != 123  -- Otros usuarios
      AND pu1.le_gusta = TRUE
    GROUP BY u2.id
    HAVING COUNT(*) >= 2  -- Al menos 2 preferencias en com√∫n
    ORDER BY preferencias_comunes DESC
    LIMIT 10
)
SELECT us.usuario_similar, v.poi_id, v.puntuacion_general
FROM usuarios_similares us
JOIN valoraciones v ON us.usuario_similar = v.usuario_id
WHERE v.puntuacion_general >= 4.0;
```

================================================================================
10. AN√ÅLISIS DE BASES DE DATOS: ¬øSON SUFICIENTES?
================================================================================

üèóÔ∏è **EVALUACI√ìN DE LA ARQUITECTURA ACTUAL**

‚úÖ **BASES DE DATOS ACTUALES**:

1Ô∏è‚É£ **BD OPERACIONAL** (Puerto 5432):
- ‚úÖ Usuarios y preferencias
- ‚úÖ POIs completos con categor√≠as
- ‚úÖ Itinerarios y seguimiento
- ‚úÖ Valoraciones detalladas
- ‚úÖ Eventos scraped

2Ô∏è‚É£ **BD PROCESSOR** (Puerto 5433):
- ‚úÖ Datos optimizados para clustering
- ‚úÖ Features pre-calculados
- ‚úÖ M√©tricas agregadas
- ‚úÖ √çndices geoespaciales

üö® **LIMITACIONES IDENTIFICADAS**:

‚ùå **FALTA BD DE RECOMENDACIONES (CR√çTICO)**:
```sql
-- Nueva BD necesaria: baxperience_recommendations
CREATE DATABASE baxperience_recommendations;

-- Tabla para almacenar clusters pre-calculados
CREATE TABLE clusters_pois (
    id SERIAL PRIMARY KEY,
    poi_id INTEGER NOT NULL,
    cluster_geografico INTEGER,
    cluster_tematico INTEGER,
    cluster_hibrido INTEGER,
    ultima_actualizacion TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Tabla para similitudes entre usuarios
CREATE TABLE similitudes_usuarios (
    id SERIAL PRIMARY KEY,
    usuario_a INTEGER NOT NULL,
    usuario_b INTEGER NOT NULL,
    similitud_score DECIMAL(5,4),  -- 0.0 a 1.0
    similitud_preferencias DECIMAL(5,4),
    similitud_valoraciones DECIMAL(5,4),
    fecha_calculo TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    UNIQUE(usuario_a, usuario_b)
);

-- Tabla para recomendaciones pre-calculadas
CREATE TABLE recomendaciones_usuario (
    id SERIAL PRIMARY KEY,
    usuario_id INTEGER NOT NULL,
    poi_id INTEGER NOT NULL,
    score_recomendacion DECIMAL(5,4),
    razon_recomendacion TEXT,  -- 'usuarios_similares', 'clustering', 'preferencias'
    fecha_calculo TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_usuario_score (usuario_id, score_recomendacion DESC)
);
```

üîß **BASES DE DATOS ADICIONALES RECOMENDADAS**:

3Ô∏è‚É£ **BD DE RECOMENDACIONES** (Puerto 5434):
- ‚úÖ Clusters pre-calculados
- ‚úÖ Similitudes entre usuarios
- ‚úÖ Recomendaciones personalizadas
- ‚úÖ M√©tricas de efectividad

4Ô∏è‚É£ **BD DE CACHE/REDIS** (Puerto 6379):
- ‚úÖ Cache de consultas frecuentes
- ‚úÖ Sesiones de usuario
- ‚úÖ Resultados de clustering temporales
- ‚úÖ Geolocalizaci√≥n en tiempo real

**RESPUESTA: LAS DOS BD ACTUALES NO SON SUFICIENTES**

Necesitas m√≠nimo 4 bases de datos para un sistema de recomendaciones robusto:
1. BD Operacional (ya tienes) ‚úÖ
2. BD Processor (ya tienes) ‚úÖ  
3. BD Recomendaciones (FALTA) ‚ùå
4. Redis Cache (FALTA) ‚ùå

üìã **PLAN DE IMPLEMENTACI√ìN RESUMIDO**:

üöÄ **SEMANAS 1-4: Algoritmos de Clustering**
- Implementar K-Means geogr√°fico  
- Desarrollar clustering tem√°tico
- Crear sistema de collaborative filtering

üóÑÔ∏è **SEMANAS 5-8: BD de Recomendaciones**  
- Crear tercera base de datos
- Implementar cache layer con Redis
- Desarrollar API de recomendaciones

üì± **SEMANAS 9-12: Integraci√≥n y Optimizaci√≥n**
- Conectar con frontend
- Sistema de feedback en tiempo real
- M√©tricas de efectividad y A/B testing

üéØ **RESULTADO ESPERADO**: Sistema que genere itinerarios personalizados en <2 segundos con >80% de satisfacci√≥n del usuario.

================================================================================
FIN DEL AN√ÅLISIS COMPLETO - BAXperience Clustering & Recommendations System
================================================================================
